{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPQjZs8GjZjQ",
        "outputId": "04bdc290-b640-4c6c-fdb6-2c59a437b305"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/48.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.0/48.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/142.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.6/142.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/109.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.6/109.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain-groq langchain tavily-python replicate langgraph matplotlib -q\n",
        "!pip install langgraph-checkpoint-sqlite langchain_community -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Gemini API Key from [Google AI Studio](https://aistudio.google.com/app/apikey?_gl=1*13wr2n0*_ga*MTg5MjM2NTI0My4xNzM3Mzc4OTQ2*_ga_P1DBVKWT6V*MTczNzM3ODk0NS4xLjEuMTczNzM4MDE2NS42MC4wLjEyMTUxMTk2Mzg.)"
      ],
      "metadata": {
        "id": "U3f3mvUx7g8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GROQ_API_KEY=userdata.get('groq_api_key')\n",
        "GEMINI_API_KEY=userdata.get('GEMINI_API_KEY')\n",
        "=GEMINI_API_KEY\n",
        "#TAVILY_API_KEY=userdata.get('TAVILY_API_KEY')"
      ],
      "metadata": {
        "id": "kb9nXyzbjmpB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Go through [Gemini Docs](https://ai.google.dev/gemini-api/docs) for more info"
      ],
      "metadata": {
        "id": "4fgdMXfF78je"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain-google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Od0LSBTe_cP8",
        "outputId": "baa0b91b-fc79-4c86-c6a4-6d9003376c56"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] =GEMINI_API_KEY"
      ],
      "metadata": {
        "id": "QF2eNH-I_0UD"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    # other params...\n",
        ")"
      ],
      "metadata": {
        "id": "vqYTD6ox_QvS"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TRCFhWhQ_uEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    (\n",
        "        \"system\",\n",
        "        \"You are a helpful assistant for the user\",\n",
        "    ),\n",
        "    (\"human\", \" how can i transfer Larger model weights to smaller models to get better inference .\"),\n",
        "]\n",
        "llm.invoke(messages)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_aYpS1H_Wqg",
        "outputId": "4a3c916a-056c-4d8a-9ebd-9717615b2386"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Transferring knowledge from a larger model to a smaller one can significantly improve the smaller model's performance and reduce training time. This process is often called **knowledge distillation** or **model compression**. Here's a breakdown of common techniques and considerations:\\n\\n**1. Choosing the Right Transfer Method:**\\n\\n* **Fine-tuning:**  This is the most straightforward approach.  You initialize the smaller model with the weights of the larger model (specifically, the layers that correspond structurally). Then, you train the smaller model on your target dataset, allowing it to adapt to the specific task while benefiting from the pre-trained knowledge.  This works best when the smaller model has a similar architecture to the larger one.\\n\\n* **Knowledge Distillation:** This involves training the smaller model to mimic the *output distribution* of the larger model, rather than just the final classification.  The larger model's softened probabilities (often achieved through temperature scaling) provide richer information than hard labels, guiding the smaller model's learning.  This is particularly effective when the smaller model is significantly smaller than the larger one or when data is limited.\\n\\n* **Layer-wise Transfer:**  If the architectures are quite different, you might transfer weights only for specific layers (e.g., early convolutional layers in image tasks).  This requires careful consideration of the roles of different layers in both models.\\n\\n* **Intermediate Representations:**  Instead of mimicking the output, the smaller model can be trained to match the activations of intermediate layers of the larger model. This encourages the smaller model to learn similar feature representations.\\n\\n**2. Practical Steps and Considerations:**\\n\\n* **Weight Compatibility:**  The architectures need to be compatible to some extent.  You can't directly transfer weights between completely unrelated architectures.  If layers have different shapes, you'll need to adapt the weights (e.g., by truncating, expanding, or using interpolation).\\n\\n* **Freezing Layers:** Initially, you might freeze the transferred weights and train only the new layers (or the head) of the smaller model.  This helps preserve the pre-trained knowledge.  Later, you can unfreeze all layers and fine-tune the entire model.\\n\\n* **Learning Rate:** Use a lower learning rate when fine-tuning than when training from scratch.  This prevents drastic changes to the pre-trained weights.\\n\\n* **Regularization:** Techniques like dropout and weight decay can help prevent overfitting, especially when the smaller model has limited capacity.\\n\\n* **Data Augmentation:**  If your dataset is small, data augmentation can improve the smaller model's generalization ability.\\n\\n* **Temperature Scaling (for Knowledge Distillation):**  A temperature parameter *T* is used to soften the probability distribution output by the larger model.  A higher *T* produces softer probabilities.  The same temperature should be used during training the smaller model and then set back to 1 for inference.\\n\\n**3. Example using PyTorch (Knowledge Distillation):**\\n\\n```python\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\n# ... (define your large and small models: teacher_model, student_model) ...\\n\\ndef distill(teacher_model, student_model, dataloader, optimizer, temperature=2.0, alpha=0.5):\\n    teacher_model.eval()\\n    student_model.train()\\n\\n    for images, labels in dataloader:\\n        optimizer.zero_grad()\\n\\n        with torch.no_grad():\\n            teacher_outputs = teacher_model(images)\\n            teacher_probs = F.softmax(teacher_outputs / temperature, dim=1)\\n\\n        student_outputs = student_model(images)\\n        student_probs = F.log_softmax(student_outputs / temperature, dim=1)\\n\\n        distillation_loss = nn.KLDivLoss(reduction='batchmean')(student_probs, teacher_probs) * (temperature ** 2)\\n        student_loss = F.cross_entropy(student_outputs, labels)\\n\\n        loss = alpha * student_loss + (1 - alpha) * distillation_loss\\n        loss.backward()\\n        optimizer.step()\\n\\n# ... (load pre-trained weights into teacher_model) ...\\n# ... (initialize student_model) ...\\n# ... (create dataloader) ...\\n# ... (create optimizer for student_model) ...\\n\\ndistill(teacher_model, student_model, dataloader, optimizer)\\n```\\n\\n**Key Libraries:**\\n\\n* **TensorFlow/Keras:**  Provides tools for model saving, loading, and fine-tuning.\\n* **PyTorch:**  Offers similar functionalities and is often preferred for research due to its dynamic computation graph.\\n\\n\\nBy carefully selecting the appropriate transfer method and following best practices, you can effectively leverage the knowledge of larger models to improve the performance and efficiency of smaller models for inference. Remember to experiment and find the best approach for your specific task and models.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-50647755-f8cc-47fe-9779-33c5923526ae-0', usage_metadata={'input_tokens': 25, 'output_tokens': 1043, 'total_tokens': 1068, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qQoHhRBS7gSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "llm=genai.GenerativeModel(\n",
        "  model_name=\"gemini-1.5-flash\",\n",
        "  system_instruction=\"You are a helpful assistant assist with the user query.\")"
      ],
      "metadata": {
        "id": "3mFX06cJ6_jN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.generate_content(\"How does AI work?\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "_fJEAPyI8bpI",
        "outputId": "48b7046d-a6c7-4038-c463-ddad5ca7a4ff"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artificial intelligence (AI) is a broad field encompassing many techniques, but at its core, it involves creating systems that can perform tasks that typically require human intelligence.  There's no single \"how it works,\" but here's a breakdown of some key approaches:\n",
            "\n",
            "**1. Machine Learning (ML):**  This is arguably the most prevalent approach to AI today.  Instead of explicitly programming a computer to perform a task, ML involves feeding it vast amounts of data and letting it learn patterns and relationships on its own.  Different types of ML exist:\n",
            "\n",
            "* **Supervised Learning:** The algorithm is trained on a labeled dataset, meaning the data is already tagged with the correct answers.  For example, showing the algorithm thousands of pictures of cats and dogs, labeled accordingly, so it can learn to distinguish them.\n",
            "* **Unsupervised Learning:** The algorithm is trained on unlabeled data and tries to find patterns and structures on its own.  For example, clustering similar customers based on their purchasing history.\n",
            "* **Reinforcement Learning:** The algorithm learns through trial and error, receiving rewards for correct actions and penalties for incorrect ones.  This is often used in robotics and game playing.\n",
            "\n",
            "**2. Deep Learning (DL):** A subfield of ML that uses artificial neural networks with multiple layers (hence \"deep\").  These networks are inspired by the structure and function of the human brain, allowing them to learn complex patterns from vast datasets.  Deep learning excels in areas like image recognition, natural language processing, and speech recognition.\n",
            "\n",
            "**3. Expert Systems:** These AI systems emulate the decision-making ability of a human expert in a specific domain.  They are based on a knowledge base of rules and facts, and an inference engine to apply these rules to new situations.  While effective in narrow domains, they are less adaptable than ML or DL.\n",
            "\n",
            "**4. Natural Language Processing (NLP):**  This focuses on enabling computers to understand, interpret, and generate human language.  It's used in chatbots, machine translation, and sentiment analysis.\n",
            "\n",
            "**5. Computer Vision:** This allows computers to \"see\" and interpret images and videos.  It's used in self-driving cars, medical image analysis, and facial recognition.\n",
            "\n",
            "\n",
            "**In simpler terms:** Imagine teaching a dog a trick.  With traditional programming, you'd write a precise set of instructions.  With machine learning, you'd show the dog many examples of the trick, rewarding it for success and correcting errors. Deep learning would be like showing the dog many videos of other dogs performing the trick, letting it learn the nuances on its own.\n",
            "\n",
            "\n",
            "**Limitations:**  AI systems are not truly intelligent in the human sense.  They are powerful tools capable of amazing feats, but they lack consciousness, common sense reasoning, and the ability to adapt to completely novel situations without extensive retraining.  They can also be biased if the data they are trained on is biased.\n",
            "\n",
            "\n",
            "This explanation provides a high-level overview. Each of these areas (ML, DL, NLP, etc.) has extensive subfields and complexities that require significant study to fully understand.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "from datetime import datetime\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# LLM chain\n",
        "llm = ChatGroq(temperature=0, model=\"llama3-8b-8192\",api_key=GROQ_API_KEY)"
      ],
      "metadata": {
        "id": "jgeWNcTKrYxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output=llm.invoke(\"hi i want you to write a python program for decorators\")\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7h5vKX7rZfn",
        "outputId": "9133c6ab-ad3c-4016-fd35-0d8192a1f401"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Here\\'s a simple example of a decorator in Python:\\n\\n```\\ndef my_decorator(func):\\n    def wrapper():\\n        print(\"Something is happening before the function is called.\")\\n        func()\\n        print(\"Something is happening after the function is called.\")\\n    return wrapper\\n\\n@my_decorator\\ndef say_hello():\\n    print(\"Hello!\")\\n\\nsay_hello()\\n```\\n\\nWhen you run this program, it will print:\\n\\n```\\nSomething is happening before the function is called.\\nHello!\\nSomething is happening after the function is called.\\n```\\n\\nThe `my_decorator` function is a decorator. It takes a function (`func`) as an argument, and returns a new function (`wrapper`). The `wrapper` function calls the original function (`func`) and adds some extra behavior before and after the function is called.\\n\\nThe `@my_decorator` line before the `say_hello` function definition is a syntax for applying the decorator to the function. It\\'s equivalent to writing `say_hello = my_decorator(say_hello)`.\\n\\nHere\\'s another example of a decorator that takes an argument:\\n\\n```\\ndef repeat(num_times):\\n    def decorator(func):\\n        def wrapper():\\n            for i in range(num_times):\\n                func()\\n        return wrapper\\n    return decorator\\n\\n@repeat(3)\\ndef say_hello():\\n    print(\"Hello!\")\\n\\nsay_hello()\\n```\\n\\nWhen you run this program, it will print:\\n\\n```\\nHello!\\nHello!\\nHello!\\n```\\n\\nIn this example, the `repeat` function is a decorator factory. It takes an argument (`num_times`) and returns a decorator. The decorator is then applied to the `say_hello` function, which is called `num_times` times.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 337, 'prompt_tokens': 21, 'total_tokens': 358, 'completion_time': 0.280833333, 'prompt_time': 0.004502556, 'queue_time': 0.017422413999999997, 'total_time': 0.285335889}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_a97cfe35ae', 'finish_reason': 'stop', 'logprobs': None}, id='run-801a1b9b-b93a-4c2c-98d2-639a520fa1e4-0', usage_metadata={'input_tokens': 21, 'output_tokens': 337, 'total_tokens': 358})"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [(\"system\", \"You are a helpful translator. Translate the user sentence to French.\"),\n",
        "    (\"human\", \"I love programming.\"),]\n",
        "llm.invoke(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQHHMQI2sjO4",
        "outputId": "24627169-ab76-40e4-9dd3-2269c44a633b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Je adore le programmation.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 32, 'total_tokens': 39, 'completion_time': 0.005833333, 'prompt_time': 0.00384065, 'queue_time': 0.018435959999999998, 'total_time': 0.009673983}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_179b0f92c9', 'finish_reason': 'stop', 'logprobs': None}, id='run-df758f58-e1a8-4943-9391-3e87a1b32d25-0', usage_metadata={'input_tokens': 32, 'output_tokens': 7, 'total_tokens': 39})"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system = \"You are a helpful assistant.\"\n",
        "human = \"{text}\"\n",
        "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
        "\n",
        "chain = prompt | llm\n",
        "chain.invoke({\"text\": \"Explain the importance of low latency LLMs.\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hxLDjmoog3Y",
        "outputId": "a9f8d868-3b55-4e7f-81db-756e26c0c210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Large Language Models (LLMs) have revolutionized the field of natural language processing (NLP) by enabling machines to understand and generate human-like language. Low latency LLMs are a crucial aspect of this technology, and their importance cannot be overstated. Here are some reasons why:\\n\\n1. **Real-time applications**: Low latency LLMs are essential for real-time applications such as chatbots, virtual assistants, and language translation services. They enable instant responses to user queries, making interactions more seamless and efficient.\\n2. **Improved user experience**: Fast response times are critical for maintaining user engagement and satisfaction. Low latency LLMs ensure that users receive quick and accurate responses, reducing frustration and increasing the likelihood of repeat interactions.\\n3. **Competitive advantage**: In today's fast-paced digital landscape, companies that can deliver low-latency language processing capabilities can gain a significant competitive advantage. This is particularly important in industries like customer service, where speed and accuracy are paramount.\\n4. **Scalability**: Low latency LLMs can handle a large volume of requests simultaneously, making them ideal for large-scale applications. This scalability enables companies to process a vast amount of data and provide services to a vast user base.\\n5. **Enhanced decision-making**: Low latency LLMs can provide instant insights and recommendations, enabling businesses to make data-driven decisions quickly. This is particularly important in industries like finance, healthcare, and e-commerce, where timely decision-making can have a significant impact on outcomes.\\n6. **Reduced costs**: Low latency LLMs can reduce the need for human intervention, which can lead to cost savings. By automating language processing tasks, companies can allocate resources more efficiently and reduce the risk of human error.\\n7. **Improved accuracy**: Low latency LLMs can process large amounts of data quickly, enabling them to learn from their mistakes and improve their accuracy over time. This self-improvement capability is critical for achieving high levels of language understanding and generation.\\n8. **Edge computing**: Low latency LLMs can be deployed at the edge of the network, closer to the user, reducing latency and improving response times. This is particularly important for applications that require low latency, such as autonomous vehicles or smart home devices.\\n9. **Increased accessibility**: Low latency LLMs can enable language processing capabilities for people with disabilities, such as those with speech or hearing impairments. This increased accessibility can have a significant impact on social inclusion and equality.\\n10. **Future-proofing**: As the demand for language processing capabilities continues to grow, low latency LLMs are essential for future-proofing applications and services. They will enable companies to stay ahead of the curve and adapt to emerging trends and technologies.\\n\\nIn summary, low latency LLMs are crucial for a wide range of applications and industries. They offer numerous benefits, including improved user experience, competitive advantage, scalability, and reduced costs. As the technology continues to evolve, the importance of low latency LLMs will only continue to grow.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 609, 'prompt_tokens': 32, 'total_tokens': 641, 'completion_time': 0.5075, 'prompt_time': 0.004321158, 'queue_time': 0.017576811999999997, 'total_time': 0.511821158}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'stop', 'logprobs': None}, id='run-255d82db-6edd-4211-93af-ce1b2cd88090-0', usage_metadata={'input_tokens': 32, 'output_tokens': 609, 'total_tokens': 641})"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_core.tools import tool\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "from datetime import datetime\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "class State(TypedDict):\n",
        "    # Messages have the type \"list\". The `add_messages` function\n",
        "    # in the annotation defines how this state key should be updated\n",
        "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "graph_builder = StateGraph(State)"
      ],
      "metadata": {
        "id": "JmyAnB_Xj_GV"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot(state:State):\n",
        "  return {\"messaages\": [llm.invoke(state[\"messages\"])]}\n",
        "\n",
        "## Above function works for ChatGpt and Antrhopic models\n",
        "## We need to change the prompt template for Llama models/Groq models\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "def chatbot(state:State):\n",
        "    # Define the system and human roles\n",
        "    system = \"You are a helpful assistant.\"\n",
        "    human = \"{text}\"\n",
        "    # Create a ChatPromptTemplate\n",
        "    prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
        "    # Chain the prompt with the LLaMA model\n",
        "    chain = prompt | llm\n",
        "    # Get the last message from state[\"messages\"] as input\n",
        "    last_message = state[\"messages\"][-1] if state[\"messages\"] else \"\"\n",
        "\n",
        "    # Invoke the model with the last message and return the result\n",
        "    response = chain.invoke({\"text\": last_message})\n",
        "    return {\"messages\": state[\"messages\"] + [response]}\n",
        "\n"
      ],
      "metadata": {
        "id": "vaWUAtX8jokm"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add Nodes"
      ],
      "metadata": {
        "id": "pdbbt-F9ld65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The first argument is the unique node name\n",
        "# The second argument is the function or object that will be called whenever\n",
        "# the node is used.\n",
        "graph_builder.add_node(\"chatbot\", chatbot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJkMmrBMlcY5",
        "outputId": "25026c89-43ef-46d5-d54a-f374acb66e23"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7f6dd834f950>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add Edges (define how the Logic should flow )"
      ],
      "metadata": {
        "id": "5BYO0rsilgEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Logic Flow of the Graph\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph_builder.add_edge(\"chatbot\",END)\n",
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "T5-2zfRBj-le"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "except Exception:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "Sq0rjfHzlEPm",
        "outputId": "a2519b44-c2fa-4e3a-c64a-68e0edb0d1d2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def stream_graph_updates(user_input: str):\n",
        "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
        "        for value in event.values():\n",
        "            print(\"Assistant:\", value[\"messages\"][-1].content)\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"User: \")\n",
        "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        stream_graph_updates(user_input)\n",
        "    except:\n",
        "        # fallback if input() is not available\n",
        "        user_input = \"What do you know about LangGraph?\"\n",
        "        print(\"User: \" + user_input)\n",
        "        stream_graph_updates(user_input)\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RWZhlBclIWA",
        "outputId": "73d738c4-34e4-44f4-fc29-12f2f82d2761"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: hi\n",
            "Assistant: This looks like a snippet of data representing a single message or interaction, likely within a larger system. Let's break down the components:\n",
            "\n",
            "* **`content='hi'`**: This indicates the actual content of the message. In this case, it's simply \"hi.\"\n",
            "\n",
            "* **`additional_kwargs={}`**:  `additional_kwargs` stands for \"additional keyword arguments.\"  This is an empty dictionary, meaning no extra information or metadata was passed along with the message content.  This field is likely used for flexibility, allowing developers to attach custom data as needed.\n",
            "\n",
            "* **`response_metadata={}`**: Similar to `additional_kwargs`, this is an empty dictionary for metadata related to the *response* to this message. Since it's empty, no response metadata was recorded or is available.\n",
            "\n",
            "* **`id='2984b6e9-8c3e-43f2-93ca-e35bdda2e49c'`**: This is a Universally Unique Identifier (UUID).  It serves as a unique identifier for this specific message, allowing the system to track and distinguish it from other messages.\n",
            "\n",
            "**In summary:** This data snippet represents a simple \"hi\" message within a system.  It doesn't have any additional context or associated response information, but it's uniquely identified by a UUID.  This structure is commonly used for logging, tracking, and managing interactions within applications, especially those involving asynchronous communication or message queues.\n",
            "User: What are the things should i remember before doing knowledge distillation of model weights \n",
            "Assistant: Before performing knowledge distillation of model weights, keep the following points in mind:\n",
            "\n",
            "**1. Teacher and Student Selection:**\n",
            "\n",
            "* **Teacher Model:** Choose a larger, more complex model (or an ensemble) that performs well on the target task.  It should have a higher capacity than the student.\n",
            "* **Student Model:** Select a smaller, faster model architecture that you want to train. The goal is for the student to achieve comparable performance to the teacher while being more efficient.\n",
            "* **Capacity Gap:**  A significant capacity gap between the teacher and student is generally beneficial, but too large a gap can make learning difficult for the student.\n",
            "\n",
            "**2. Data Considerations:**\n",
            "\n",
            "* **Dataset:**  You'll need the same dataset used to train the teacher model.  In some cases, augmenting the data can further improve student performance.\n",
            "* **Data Preprocessing:** Ensure consistent data preprocessing between teacher and student training.\n",
            "\n",
            "**3. Distillation Techniques:**\n",
            "\n",
            "* **Soft Targets vs. Hard Targets:**  Decide whether to use soft targets (predicted probabilities from the teacher) or hard targets (one-hot encoded ground truth labels). Soft targets provide more information about the relationships between classes and are generally preferred.\n",
            "* **Temperature:**  When using soft targets, a temperature parameter controls the softness of the probability distribution. Higher temperatures lead to softer distributions, emphasizing relationships between classes. Experiment with different temperature values.\n",
            "* **Loss Function:**  The distillation loss combines the loss between the student's predictions and the soft targets (distillation loss) and the loss between the student's predictions and the hard targets (student loss). Balance these losses appropriately.  Common choices include Kullback-Leibler (KL) divergence for the distillation loss and cross-entropy for the student loss.\n",
            "* **Intermediate Representations:**  Consider distilling knowledge from intermediate layers of the teacher, not just the final output layer. This can be particularly helpful for complex tasks.\n",
            "\n",
            "**4. Training Process:**\n",
            "\n",
            "* **Learning Rate:**  Choose an appropriate learning rate for the student model.\n",
            "* **Optimizer:**  Select a suitable optimizer (e.g., Adam, SGD).\n",
            "* **Hyperparameter Tuning:**  Experiment with different hyperparameters for both the student model and the distillation process itself (e.g., temperature, loss weights).\n",
            "* **Evaluation Metrics:**  Monitor relevant metrics (e.g., accuracy, F1-score) on a validation set to track the student's progress and prevent overfitting.\n",
            "\n",
            "**5. Practical Considerations:**\n",
            "\n",
            "* **Computational Resources:**  Distillation requires training both the teacher and student models, which can be computationally intensive.\n",
            "* **Implementation Details:**  Various deep learning frameworks offer tools and examples for knowledge distillation.  Leverage these resources to simplify the implementation process.\n",
            "* **Overfitting:**  Be mindful of potential overfitting, especially if the student model is very small or the dataset is limited.  Regularization techniques can help mitigate this.\n",
            "\n",
            "\n",
            "By carefully considering these factors, you can effectively perform knowledge distillation and train a smaller, faster model that achieves comparable performance to a larger, more complex teacher model.\n",
            "User: q\n",
            "Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eTLGPi9a9xNT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}